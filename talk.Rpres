<style>
.small-code pre code {
  font-size: 1em;
}
</style>

Basic principles of a Bayesian workflow
========================================================
author: Trent Henderson
date: 16 July 2021
autosize: true
css: corp-styles.css
transition: linear


Goals of the presentation
========================================================
class: small-code

This interactive talk aims to help you achieve three things:

- Develop an initial understanding of Bayes rule and how to apply it
- Understand how the Bayesian approach fits within the open science framework
- Understand the tools to carry out Bayesian analysis (with an example)

All the code for today is in a [GitHub repository](https://github.com/hendersontrent/bayes-workflow-pres).

```{r setup, include = FALSE}
library(knitr)
opts_chunk$set(fig.width = 8, fig.height = 4.2, dpi = 600, out.width = "850px", out.height = "450px")
```

```{r, message = FALSE, echo = FALSE}
library(tidyverse)
library(scales)
library(rstan)
library(bayesplot)
library(fitzRoy)
library(data.table)
library(janitor)
library(tidybayes)
library(modelr)

# Named palette of colours to ensure they get consistent colours in all plots
palette <- c("#E494D3", "#87DCC0", "#88BBE4", "#998AD3")
names(palette) <- c("Prior", 
                    "Likelihood of seeing 5/10 students in Go8",
                    "Unstandardised Posterior",
                    "Standardised Posterior",
                    "Random Sample of Students")
```

Activity: Our prior beliefs
========================================================

## Consider the following question:

**What proportion of all university students in Australia are studying at a Go8?**

* 0.2
* 0.3
* 0.4

Pop your answers in the Teams chat!

Activity: Our prior belief
========================================================
class: small-code

Since we have a few values that are close together, we believe that the **true** proportion could be most likely somewhere around 0.3 but with some variability. We can model this uncertainty using a distribution. `Beta` distributions are ideal for proportion outcomes.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
x <- seq(0, 1, length.out = 11)
the_xlab <- "Possible values of actual proportion"

pr <- data.frame(x = x,
                 y = dbeta(x, shape1 = 3.5, shape2 = 6.5),
                 category = "Prior") %>%
  mutate(y = y / sum(y))

pr %>%
  ggplot(aes(x = x, y = y)) +
  geom_line(size = 1.25) +
  labs(title = "Our prior",
       x = the_xlab,
       y = "Probability Density")
```

Activity: A sample of data
========================================================
class: small-code

Now let's say we sampled 10 university students and observed whether they were attending a Go8 or not and 5 said they were.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
lh <- data.frame(x = 0:10) %>%
  mutate(y = dbinom(x = x, prob = 0.5, size = 10),
         category = names(palette)[2]) %>%
  mutate(x = x / max(x))

pr %>%
  ggplot(aes(x = x, y = y)) +
  geom_line(aes(colour = category), size = 1.25) +
  geom_line(data = lh, size = 1.25, aes(colour = category)) +
  labs(title = "Our prior and a random sample of students",
       x = the_xlab,
       y = "Probability Density",
       colour = NULL) +
  scale_colour_manual(values = palette) +
  theme(legend.position = "bottom")
```

Activity: Combining our belief and the observed data
========================================================
class: small-code

We can multiply our `prior` by the observed data (`likelihood`) to get the `posterior`.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
posterior <- data.frame(x = x,
                        y = pr$y*lh$y,
                        category = "Unstandardised Posterior")

pr %>%
  ggplot(aes(x = x, y = y)) +
  geom_line(aes(colour = category), size = 1.25) +
  geom_line(data = lh, size = 1.25, aes(colour = category)) +
  geom_line(data = posterior, size = 1.25, aes(colour = category)) +
  labs(title = "Our prior, random sample, and posterior update",
       x = the_xlab,
       y = "Probability Density",
       colour = NULL) +
  scale_colour_manual(values = palette) +
  theme(legend.position = "bottom")
```

Activity: Updating our beliefs
========================================================
class: small-code

To properly update our beliefs, we need to standardise our posterior so that the total probability equals one.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
st_post <- posterior %>%
  mutate(y = y / sum(y),
         category = "Standardised Posterior")

pr %>%
  ggplot(aes(x = x, y = y)) +
  geom_line(aes(colour = category), size = 1.25) +
  geom_line(data = lh, size = 1.25, aes(colour = category)) +
  geom_line(data = posterior, size = 1.25, aes(colour = category)) +
  geom_line(data = st_post, size = 1.25, aes(colour = category)) +
  labs(title = "Our prior, random sample, and standardised posterior update",
       x = the_xlab,
       y = "Probability Density",
       colour = NULL) +
  scale_colour_manual(values = palette) +
  theme(legend.position = "bottom")
```

Activity: The impact of sample size
========================================================
class: small-code

So far we have used a random sample of 10 students. But what happens if we sample 100 and 50% still said they were at a Go8?

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
do_bayes <- function(n = 100){
  
  # Prior
  
  x <- seq(0, 1, length.out = n+1)
  
  pr <- data.frame(x = x,
                     y = dbeta(x, shape1 = 3.5, shape2 = 6.5),
                     category = "Prior") %>%
    mutate(y = y / sum(y))
  
  # Likelihood
  
  lh <- data.frame(x = 0:n) %>%
    mutate(y = dbinom(x = x, prob = 0.5, size = n),
           category = "Random Sample of Students",
           x = x / n)
  
  # Posterior
  
  posterior <- data.frame(x = x,
                          y = pr$y*lh$y,
                          category = "Unstandardised Posterior")
  
  st_post <- posterior %>%
    mutate(y = y / sum(y),
           category = "Standardised Posterior")
  
  p <- pr %>%
    ggplot(aes(x = x, y = y)) +
    geom_line(aes(colour = category), size = 1.25) +
    geom_line(data = lh, size = 1.25, aes(colour = category)) +
    geom_line(data = st_post, size = 1.25, aes(colour = category)) +
    labs(title = "Our prior, random sample, and posterior update",
         x = the_xlab,
         y = "Probability Density",
         colour = NULL) +
    scale_colour_manual(values = palette) +
    theme(legend.position = "bottom")
  
  return(p)
}

p_5 <- do_bayes(n = 5) + labs(subtitle = "Sample = 5")
p_10 <- do_bayes(n = 10) + labs(subtitle = "Sample = 10")
p_100 <- do_bayes(n = 100) + labs(subtitle = "Sample = 100")
p_1000 <- do_bayes(n = 1000) + labs(subtitle = "Sample = 1000")
```

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
print(p_100)
```

Activity: The impact of sample size
========================================================
class: small-code

How about 1000? As sample size increases, the impact of the prior on the posterior weakens in comparison to the data/likelihood.

```{r, message = FALSE, warning = FALSE, echo = FALSE, fig.keep = TRUE}
print(p_1000)
```

The mathematics of Bayesian statistics
========================================================

Bayesian statistics boils down to [Bayes's Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):

$P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}$

Let's break it down formally:

$P(\theta \mid D)$ - this is called the **posterior** (probability of model parameters given the data)

$P(D \mid \theta)$ - this is called the **likelihood** (probability of the data given model parameters)

$P(\theta)$ - this is called the **prior** (our expressed understanding of the probability of model parameters)

$P(D)$ - this is called the **marginal likelihood** (probability of the data)

Mathematical complications
========================================================

The **marginal likelihood** (denominator in Bayes Theorem) is the reason we can't just compute complex Bayesian models easily - it involves summing (integrating) over all the possible values of the distributions. In a trivial single number case it is easy to just add the numbers, but when using higher-dimensional models and complicated prior and likelihood distributions, this becomes analytically impossible.

To get around this, we instead employ sampling algorithms, such as [Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo), to simulate a large number of times to approximate the posterior distribution instead.

Benefits of a Bayesian approach to inference
========================================================

* Probabilistic estimates
* Quantification of uncertainty
* Ability to capture subject matter expertise
* Intuitive conceptualisation of statistics
* Simulation can help address limitations with small N
* No *p*-values (don't get me started on Bayes factors...)

Connection to Registered Report format
========================================================

![Centre for Open Science](registered_reports.width-800.png)

A Bayesian framework (while not perfect) protects against a lot of traditional frequentist issues such as *p*-hacking. Bayesian formalism integrates easily into Registered Reports format as Bayesian inference is largely concerned with transparently *modelling the underlying statistical process that is likely to have generated your data*. This forces researchers to think deeper about study design and analysis methodology, consistent with the RR format.

The overall workflow
========================================================

INSERT DIAGRAM HERE

Tools for Bayesian inference
========================================================

INSERT DIAGRAM HERE

An applied example
========================================================

Now that we have the basics, let's take a look at a basic Bayesian logistic regression in R using `Stan`.

## The premise

**We are going to explore whether time series features can classify .**

Example: Data preparation
========================================================
class: small-code

Data extraction using the `fitzRoy` package.

```{r, message = FALSE, warning = FALSE}

```

Example: Data aggregation
========================================================
class: small-code

```{r, message = FALSE}
'%ni%' <- Negate('%in%')
the_finals <- c("EF", "SF", "QF", "PF", "GF") # Remove finals as these might influence analysis

# Aggregate data

d <- all_seasons %>%
  filter(round %ni% the_finals) %>%
  mutate(uniqueid = paste0(season,"_",round,"_",home_team,"_",away_team)) %>%
  group_by(season, round, playing_for, uniqueid) %>%
  summarise(goals = sum(goals),
            marks_inside_50 = sum(marks_inside_50)) %>%
  ungroup()
```

Example: Prior specification
========================================================
class: small-code

Since I have no real clue how many goals there'd be if there were zero marks, I have a really vague prior for the intercept:

```{r, message = FALSE, echo = FALSE, fig.keep = TRUE, warning = FALSE}
set.seed(123)

alpha_mine <- data.frame(x = rnorm(1000, mean = 0, sd = 5),
                         category = "Initial Prior")
am <- ggplot() +
  geom_density(data = alpha_mine, aes(x = x, fill = category), alpha = 0.4, colour = "black") +
  labs(title = "Intercept",
       x = "Value",
       y = "Density",
       fill = NULL) +
  theme(legend.position = "bottom")

print(am)
```

Example: Basic visualisation
========================================================
class: small-code

But I am confident that more marks means more goals, but not 1:1! Might be 10 times as many marks as goals, which means 0.1 as a slope? So here is my vague prior for the regression coefficient.

```{r, message = FALSE, echo = FALSE, fig.keep = TRUE, warning = FALSE}
set.seed(123)

beta_mine <- data.frame(x = rnorm(1000, mean = 0.1, sd = 0.05),
                        category = "Initial Prior")

bm <- ggplot() +
  geom_density(data = beta_mine, aes(x = x, fill = category), alpha = 0.4, colour = "black") +
  labs(title = "Coefficient",
       x = "Value",
       y = "Density",
       fill = NULL) +
  theme(legend.position = "bottom")

print(bm)
```

Example: Initial Bayesian model fit
========================================================
class: small-code

We can fit a model on the data with my pretty vague priors to get started. Note the familiar syntax to your standard `glm` in `R`.

```{r, warning = FALSE, message = FALSE, results = 'hide'}
historical <- d %>%
  filter(season != 2020)

m1 <- stan_glm(goals ~ marks_inside_50,
               data = historical, family = neg_binomial_2,
               prior = normal(0.1,0.05), prior_intercept = normal(0, 5),
               chains = 3, seed = 123)
```

Example: Initial Bayesian model fit
========================================================
class: small-code

We can compare our prior with the posterior. Here is the intercept.

```{r, message = FALSE, echo = FALSE, fig.keep = TRUE, warning = FALSE}
historical_posterior <- as.data.frame(m1) %>%
  clean_names() %>%
  mutate(category = "Historical Posterior")

am <- am +
  geom_density(data = historical_posterior, aes(x = intercept, fill = category), colour = "black")

print(am)
```

Example: Initial Bayesian model fit
========================================================
class: small-code

And here is the slope coefficient, which is much more interesting.

```{r, message = FALSE, echo = FALSE, fig.keep = TRUE, warning = FALSE}
bm <- bm +
  geom_density(data = historical_posterior, aes(x = marks_inside_50, fill = category), colour = "black")

print(bm)
```

Example: Using historical posterior as new prior
========================================================
class: small-code

We can now extract the posterior information from the historical model to use as the 2020 model prior.

```{r, warning = FALSE, message = FALSE, results = 'hide'}
hist_post_agg <- as.data.frame(m1) %>%
  clean_names() %>%
  summarise(alpha_mean = mean(intercept),
            alpha_sd = sd(intercept),
            beta_mean = mean(marks_inside_50),
            beta_sd = sd(marks_inside_50))

season2020 <- d %>%
  filter(season == 2020)

m2 <- stan_glm(goals ~ marks_inside_50,
               data = season2020, family = neg_binomial_2,
               prior = normal(location = hist_post_agg$beta_mean, scale = hist_post_agg$beta_sd),
               prior_intercept = normal(location = hist_post_agg$alpha_mean, scale = hist_post_agg$alpha_sd),
               chains = 3, seed = 123)
```

Example: Using historical posterior as new prior
========================================================
class: small-code

And a final comparison of our initial prior, the historical data posterior (which became the 2020 model prior) and the 2020 posterior. We are skipping the intercept as it is of little interest, so let's just look at the regression coefficient.

```{r, message = FALSE, echo = FALSE, fig.keep = TRUE, warning = FALSE}
posterior_2020 <- as.data.frame(m2) %>%
  clean_names() %>%
  mutate(category = "2020 Posterior")

bm <- bm +
  geom_density(data = posterior_2020, aes(x = marks_inside_50, fill = category), 
               colour = "black") +
  coord_cartesian(xlim = c(0.01, 0.06))

print(bm)
```

Example: Model diagnostics
========================================================
class: small-code

We can check if the chains mixed in one line of `R` code. We want these just to look like white noise, which they do here.

```{r, warning = FALSE, message = FALSE}
color_scheme_set("mix-blue-pink")
mcmc_trace(m2, facet_args = list(nrow = 2, labeller = label_parsed))
```

Example: Model diagnostics
========================================================
class: small-code

We can further test how well our simulated data tracks against the real data.

```{r, warning = FALSE, message = FALSE}
ppc_dens_overlay(y = m2$y,
                 yrep = posterior_predict(m2, draws = 100))
```

Example: Plotting posteriors automatically
========================================================
class: small-code

Here is the coefficient posterior with shaded 80% credible intervals. Credible intervals specify the probability with which the true value lies (compared to 'confidence intervals' which are *not* probability distributions).

```{r, warning = FALSE, message = FALSE}
mcmc_areas(m2, pars = "marks_inside_50", prob = 0.8)
```

Example: Our model against the actual data
========================================================
class: small-code

```{r, warning = FALSE, message = FALSE}
season2020 %>%
  data_grid(marks_inside_50 = modelr::seq_range(marks_inside_50, n = nrow(season2020)), goals) %>%   
  add_fitted_draws(m2, n = 100) %>%
  ggplot(aes(x = marks_inside_50, y = goals)) +
  geom_jitter(data = season2020, colour = nous_colour("C3"), position = "jitter")+
  geom_line(aes(y = .value, group = .draw),alpha = 0.1,color = nous_colour("S5"))+
  labs(title = "100 random posterior draws for 2020 data",x = "Marks Inside 50",
       y = "Goals")
```

Common criticisms of Bayesian inference
========================================================

* Computation time
* "Subjectivity" of priors

Final remarks
========================================================

There is much, much more to learn in Bayesian statistics and many other ways to evaluate models. This session hopefully served as a primer to either inspire you to learn more, or to at least consider using Bayesian approaches on current/future projects.

Special thanks to Ben Fulcher for providing input on the content of the talk, and [Peter Ellis](http://freerangestats.info) for providing input on the first half of the talk for the version I originally gave to my firm in early 2021.

**Using Bayes' Theorem doesn't make you a Bayesian. Quantifying uncertainty with probability makes you a Bayesian** - Michael Betancourt